{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwQ02MTW89rY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d11d5e-cfb1-4a91-ac90-cba93b33acb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "import cv2\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/data/car_racing_rollouts_5.pkl\"\n",
        "with open(data_path, \"rb\") as f:\n",
        "    rollouts = pickle.load(f)\n",
        "print(\"Dataset loaded successfully!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "xeBWDp2iWb-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),   # -> [32, 32, 32]\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # -> [64, 16, 16]\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), # -> [128, 8, 8]\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),# -> [256, 4, 4]\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
        "\n",
        "        self.fc_decode = nn.Linear(latent_dim, 256 * 4 * 4)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # -> [128, 8, 8]\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # -> [64, 16, 16]\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),   # -> [32, 32, 32]\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),    # -> [1, 64, 64]\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        mu = self.fc_mu(x)\n",
        "        logvar = self.fc_logvar(x)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)  # standard deviation\n",
        "        eps = torch.randn_like(std)    # sample from N(0,1)\n",
        "        return mu + std * eps          # z = mu + sigma * epsilon\n",
        "\n",
        "    def decode(self, z):\n",
        "        x = self.fc_decode(z)\n",
        "        x = x.view(-1, 256, 4, 4)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_recon = self.decode(z)\n",
        "        return x_recon, mu, logvar\n",
        "\n"
      ],
      "metadata": {
        "id": "UtyyYbUWK_S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 64\n",
        "vae = VAE(latent_dim).to(device)\n",
        "vae.load_state_dict(torch.load(\"/content/drive/MyDrive/vae_model_epoch_20.pth\", map_location=device))\n",
        "vae.eval()\n",
        "print(\"VAE loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1xwI9GGVwB1",
        "outputId": "c713995e-3d90-466b-aa5b-c4c30c9c9d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-8b7ece458292>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vae.load_state_dict(torch.load(\"/content/drive/MyDrive/vae_car_racing.pth\", map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAE loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MDNRNN(nn.Module):\n",
        "    def __init__(self, latent_dim=32, action_dim=3, hidden_dim=256, n_gaussians=5):\n",
        "        super(MDNRNN, self).__init__()\n",
        "        self.input_dim = latent_dim + action_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_gaussians = n_gaussians\n",
        "        self.lstm = nn.LSTM(self.input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_pi = nn.Linear(hidden_dim, n_gaussians)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, n_gaussians * latent_dim)\n",
        "        self.fc_sigma = nn.Linear(hidden_dim, n_gaussians * latent_dim)\n",
        "        self.fc_reward = nn.Linear(hidden_dim, 1)  # reward prediction\n",
        "    def forward(self, x, hidden=None):\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        pi = self.fc_pi(out)\n",
        "        pi = nn.functional.softmax(pi, dim=-1)\n",
        "        mu = self.fc_mu(out)\n",
        "        mu = mu.view(x.size(0), x.size(1), self.n_gaussians, -1)\n",
        "        sigma = self.fc_sigma(out)\n",
        "        sigma = sigma.view(x.size(0), x.size(1), self.n_gaussians, -1)\n",
        "        sigma = torch.exp(sigma)\n",
        "        reward = self.fc_reward(out)\n",
        "        return pi, mu, sigma, reward, hidden\n"
      ],
      "metadata": {
        "id": "sinxdmxU9tmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mdn_rnn = MDNRNN(latent_dim=latent_dim, action_dim=3, hidden_dim=256, n_gaussians=5).to(device)\n",
        "optimizer = optim.Adam(mdn_rnn.parameters(), lr=1e-3)\n",
        "print(\"MDN-RNN model initialized.\")\n",
        "\n",
        "\n",
        "def mdn_loss(pi, mu, sigma, target):\n",
        "\n",
        "    target = target.unsqueeze(2)\n",
        "    exponent = -0.5 * ((target - mu) / sigma)**2\n",
        "    exponent = exponent.sum(dim=-1)  # sum over latent_dim\n",
        "    latent_dim = target.size(-1)\n",
        "    log_coef = - torch.log(sigma).sum(dim=-1) - 0.5 * latent_dim * np.log(2 * np.pi)\n",
        "    log_probs = log_coef + exponent  # log probability for each mixture\n",
        "    weighted_log_probs = torch.log(pi + 1e-8) + log_probs\n",
        "    max_log, _ = torch.max(weighted_log_probs, dim=-1, keepdim=True)\n",
        "    log_sum = max_log.squeeze(-1) + torch.log(torch.sum(torch.exp(weighted_log_probs - max_log), dim=-1))\n",
        "    loss = -log_sum.mean()\n",
        "    return loss\n",
        "\n",
        "# Reward loss: simple MSE.\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "seq_len = 50\n",
        "class MDNRNNDataset(Dataset):\n",
        "    def __init__(self, rollouts, vae, seq_len=50):\n",
        "        self.sequences = []\n",
        "        self.seq_len = seq_len\n",
        "        self.vae = vae\n",
        "        for episode in rollouts:\n",
        "            if len(episode) < seq_len + 1:\n",
        "                continue\n",
        "            obs_seq, action_seq, reward_seq = [], [], []\n",
        "            for transition in episode:\n",
        "                obs, action, reward, next_obs, done = transition\n",
        "                obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
        "                with torch.no_grad():\n",
        "                    mu, _ = vae.encode(obs_tensor)\n",
        "                z = mu.squeeze(0).cpu().numpy()\n",
        "                obs_seq.append(z)\n",
        "                action_seq.append(np.array(action).flatten())\n",
        "                reward_seq.append(reward)\n",
        "            for i in range(0, len(episode) - seq_len):\n",
        "                inp_seq = []\n",
        "                target_z_seq = []\n",
        "                target_reward_seq = []\n",
        "                for t in range(i, i+seq_len):\n",
        "                    inp_seq.append(np.concatenate([obs_seq[t], action_seq[t]]))\n",
        "                    target_z_seq.append(obs_seq[t+1])\n",
        "                    target_reward_seq.append(reward_seq[t])\n",
        "                self.sequences.append((np.array(inp_seq), np.array(target_z_seq), np.array(target_reward_seq)))\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    def __getitem__(self, idx):\n",
        "        inp, target_z, target_reward = self.sequences[idx]\n",
        "        return (torch.tensor(inp, dtype=torch.float32),\n",
        "                torch.tensor(target_z, dtype=torch.float32),\n",
        "                torch.tensor(target_reward, dtype=torch.float32))\n",
        "\n",
        "dataset = MDNRNNDataset(rollouts, vae, seq_len=seq_len)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "print(\"MDN-RNN dataset prepared. Total sequences:\", len(dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtBXBDUFa13A",
        "outputId": "c872e5dc-081a-47fd-bf57-5ad43f6d8129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MDN-RNN model initialized.\n",
            "MDN-RNN dataset prepared. Total sequences: 171765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "mdn_rnn.train()\n",
        "for epoch in range(1, epochs+1):\n",
        "    total_loss = 0\n",
        "    reward_loss = 0\n",
        "    for inp, target_z, target_reward in dataloader:\n",
        "        inp = inp.to(device)           # (batch, seq_len, 35)\n",
        "        target_z = target_z.to(device) # (batch, seq_len, 32)\n",
        "\n",
        "        target_reward = target_reward.to(device).unsqueeze(-1)  # (batch, seq_len, 1)\n",
        "        optimizer.zero_grad()\n",
        "        pi, mu, sigma, reward_pred, _ = mdn_rnn(inp)\n",
        "        loss_z = mdn_loss(pi, mu, sigma, target_z)\n",
        "        loss_reward = mse_loss(reward_pred, target_reward)\n",
        "        loss = loss_z + loss_reward\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        reward_loss += loss_reward\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_reward = reward_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch}, Average Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Epoch {epoch}, reward Loss: {avg_reward:.4f}\")\n",
        "\n",
        "torch.save(mdn_rnn.state_dict(), \"/content/drive/MyDrive/mdn_rnn_carla.pth\")\n",
        "print(\"MDN-RNN model saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF1N0ybga6QA",
        "outputId": "63a33522-bc35-42b1-8c0c-e952db14317b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: -72.0277\n",
            "Epoch 1, reward Loss: 0.1936\n",
            "Epoch 2, Average Loss: -93.1820\n",
            "Epoch 2, reward Loss: 0.1902\n",
            "Epoch 3, Average Loss: -98.5166\n",
            "Epoch 3, reward Loss: 0.1895\n",
            "Epoch 4, Average Loss: -107.5676\n",
            "Epoch 4, reward Loss: 0.1892\n",
            "Epoch 5, Average Loss: -112.6986\n",
            "Epoch 5, reward Loss: 0.1886\n",
            "Epoch 6, Average Loss: -115.0753\n",
            "Epoch 6, reward Loss: 0.1880\n",
            "Epoch 7, Average Loss: -118.3909\n",
            "Epoch 7, reward Loss: 0.1876\n",
            "Epoch 8, Average Loss: -123.1056\n",
            "Epoch 8, reward Loss: 0.1874\n",
            "Epoch 9, Average Loss: -124.0644\n",
            "Epoch 9, reward Loss: 0.1872\n",
            "Epoch 10, Average Loss: -123.5554\n",
            "Epoch 10, reward Loss: 0.1871\n",
            "Epoch 11, Average Loss: -126.2563\n",
            "Epoch 11, reward Loss: 0.1869\n",
            "Epoch 12, Average Loss: -127.0567\n",
            "Epoch 12, reward Loss: 0.1868\n",
            "Epoch 13, Average Loss: -129.7704\n",
            "Epoch 13, reward Loss: 0.1866\n",
            "Epoch 14, Average Loss: -128.6895\n",
            "Epoch 14, reward Loss: 0.1865\n",
            "Epoch 15, Average Loss: -130.4500\n",
            "Epoch 15, reward Loss: 0.1864\n",
            "Epoch 16, Average Loss: -132.8651\n",
            "Epoch 16, reward Loss: 0.1862\n",
            "Epoch 17, Average Loss: -133.8852\n",
            "Epoch 17, reward Loss: 0.1861\n",
            "Epoch 18, Average Loss: -132.6722\n",
            "Epoch 18, reward Loss: 0.1859\n",
            "Epoch 19, Average Loss: -134.3131\n",
            "Epoch 19, reward Loss: 0.1858\n",
            "Epoch 20, Average Loss: -134.9860\n",
            "Epoch 20, reward Loss: 0.1856\n",
            "Epoch 21, Average Loss: -134.3230\n",
            "Epoch 21, reward Loss: 0.1855\n",
            "Epoch 22, Average Loss: -136.9440\n",
            "Epoch 22, reward Loss: 0.1854\n",
            "Epoch 23, Average Loss: -136.2299\n",
            "Epoch 23, reward Loss: 0.1853\n",
            "Epoch 24, Average Loss: -140.7743\n",
            "Epoch 24, reward Loss: 0.1852\n",
            "Epoch 25, Average Loss: -140.8415\n",
            "Epoch 25, reward Loss: 0.1851\n",
            "Epoch 26, Average Loss: -139.1222\n",
            "Epoch 26, reward Loss: 0.1850\n",
            "Epoch 27, Average Loss: -142.3646\n",
            "Epoch 27, reward Loss: 0.1849\n",
            "Epoch 28, Average Loss: -144.6352\n",
            "Epoch 28, reward Loss: 0.1849\n",
            "Epoch 29, Average Loss: -145.3448\n",
            "Epoch 29, reward Loss: 0.1848\n",
            "Epoch 30, Average Loss: -145.9050\n",
            "Epoch 30, reward Loss: 0.1848\n",
            "Epoch 31, Average Loss: -143.8192\n",
            "Epoch 31, reward Loss: 0.1847\n",
            "Epoch 32, Average Loss: -146.8727\n",
            "Epoch 32, reward Loss: 0.1847\n",
            "Epoch 33, Average Loss: -147.0979\n",
            "Epoch 33, reward Loss: 0.1847\n",
            "Epoch 34, Average Loss: -147.3115\n",
            "Epoch 34, reward Loss: 0.1846\n",
            "Epoch 35, Average Loss: -147.6752\n",
            "Epoch 35, reward Loss: 0.1846\n",
            "Epoch 36, Average Loss: -147.6013\n",
            "Epoch 36, reward Loss: 0.1846\n",
            "Epoch 37, Average Loss: -143.2512\n",
            "Epoch 37, reward Loss: 0.1845\n",
            "Epoch 38, Average Loss: -143.3569\n",
            "Epoch 38, reward Loss: 0.1844\n",
            "Epoch 39, Average Loss: -145.1766\n",
            "Epoch 39, reward Loss: 0.1844\n",
            "Epoch 40, Average Loss: -148.1814\n",
            "Epoch 40, reward Loss: 0.1845\n",
            "Epoch 41, Average Loss: -148.2165\n",
            "Epoch 41, reward Loss: 0.1845\n",
            "Epoch 42, Average Loss: -148.3373\n",
            "Epoch 42, reward Loss: 0.1845\n",
            "Epoch 43, Average Loss: -148.7057\n",
            "Epoch 43, reward Loss: 0.1844\n",
            "Epoch 44, Average Loss: -149.1487\n",
            "Epoch 44, reward Loss: 0.1844\n",
            "Epoch 45, Average Loss: -149.2807\n",
            "Epoch 45, reward Loss: 0.1844\n",
            "Epoch 46, Average Loss: -148.8030\n",
            "Epoch 46, reward Loss: 0.1844\n",
            "Epoch 47, Average Loss: -149.4937\n",
            "Epoch 47, reward Loss: 0.1844\n",
            "Epoch 48, Average Loss: -149.8920\n",
            "Epoch 48, reward Loss: 0.1844\n",
            "Epoch 49, Average Loss: -150.1409\n",
            "Epoch 49, reward Loss: 0.1844\n",
            "Epoch 50, Average Loss: -150.2751\n",
            "Epoch 50, reward Loss: 0.1843\n",
            "Epoch 51, Average Loss: -149.8475\n",
            "Epoch 51, reward Loss: 0.1843\n",
            "Epoch 52, Average Loss: -150.5060\n",
            "Epoch 52, reward Loss: 0.1843\n",
            "Epoch 53, Average Loss: -150.4805\n",
            "Epoch 53, reward Loss: 0.1843\n",
            "Epoch 54, Average Loss: -150.5832\n",
            "Epoch 54, reward Loss: 0.1843\n",
            "Epoch 55, Average Loss: -150.8281\n",
            "Epoch 55, reward Loss: 0.1843\n",
            "Epoch 56, Average Loss: -151.0410\n",
            "Epoch 56, reward Loss: 0.1842\n",
            "Epoch 57, Average Loss: -151.1508\n",
            "Epoch 57, reward Loss: 0.1842\n",
            "Epoch 58, Average Loss: -151.2366\n",
            "Epoch 58, reward Loss: 0.1842\n",
            "Epoch 59, Average Loss: -151.3538\n",
            "Epoch 59, reward Loss: 0.1842\n",
            "Epoch 60, Average Loss: -151.4772\n",
            "Epoch 60, reward Loss: 0.1842\n",
            "Epoch 61, Average Loss: -151.5762\n",
            "Epoch 61, reward Loss: 0.1842\n",
            "Epoch 62, Average Loss: -151.7028\n",
            "Epoch 62, reward Loss: 0.1842\n",
            "Epoch 63, Average Loss: -151.7811\n",
            "Epoch 63, reward Loss: 0.1842\n",
            "Epoch 64, Average Loss: -151.7901\n",
            "Epoch 64, reward Loss: 0.1842\n",
            "Epoch 65, Average Loss: -151.9128\n",
            "Epoch 65, reward Loss: 0.1842\n",
            "Epoch 66, Average Loss: -152.0075\n",
            "Epoch 66, reward Loss: 0.1842\n",
            "Epoch 67, Average Loss: -152.0485\n",
            "Epoch 67, reward Loss: 0.1842\n",
            "Epoch 68, Average Loss: -152.2061\n",
            "Epoch 68, reward Loss: 0.1842\n",
            "Epoch 69, Average Loss: -152.3620\n",
            "Epoch 69, reward Loss: 0.1842\n",
            "Epoch 70, Average Loss: -152.3480\n",
            "Epoch 70, reward Loss: 0.1842\n",
            "Epoch 71, Average Loss: -152.4535\n",
            "Epoch 71, reward Loss: 0.1842\n",
            "Epoch 72, Average Loss: -152.2590\n",
            "Epoch 72, reward Loss: 0.1842\n",
            "Epoch 73, Average Loss: -152.5233\n",
            "Epoch 73, reward Loss: 0.1842\n",
            "Epoch 74, Average Loss: -152.5685\n",
            "Epoch 74, reward Loss: 0.1841\n",
            "Epoch 75, Average Loss: -152.6688\n",
            "Epoch 75, reward Loss: 0.1842\n",
            "Epoch 76, Average Loss: -152.7974\n",
            "Epoch 76, reward Loss: 0.1842\n",
            "Epoch 77, Average Loss: -152.8214\n",
            "Epoch 77, reward Loss: 0.1841\n",
            "Epoch 78, Average Loss: -152.8507\n",
            "Epoch 78, reward Loss: 0.1841\n",
            "Epoch 79, Average Loss: -152.9466\n",
            "Epoch 79, reward Loss: 0.1841\n",
            "Epoch 80, Average Loss: -152.9529\n",
            "Epoch 80, reward Loss: 0.1841\n",
            "Epoch 81, Average Loss: -153.0888\n",
            "Epoch 81, reward Loss: 0.1841\n",
            "Epoch 82, Average Loss: -153.1359\n",
            "Epoch 82, reward Loss: 0.1841\n",
            "Epoch 83, Average Loss: -153.1305\n",
            "Epoch 83, reward Loss: 0.1841\n",
            "Epoch 84, Average Loss: -153.1715\n",
            "Epoch 84, reward Loss: 0.1841\n",
            "Epoch 85, Average Loss: -153.2220\n",
            "Epoch 85, reward Loss: 0.1841\n",
            "Epoch 86, Average Loss: -153.2603\n",
            "Epoch 86, reward Loss: 0.1841\n",
            "Epoch 87, Average Loss: -153.3613\n",
            "Epoch 87, reward Loss: 0.1841\n",
            "Epoch 88, Average Loss: -152.9165\n",
            "Epoch 88, reward Loss: 0.1841\n",
            "Epoch 89, Average Loss: -153.4654\n",
            "Epoch 89, reward Loss: 0.1841\n",
            "Epoch 90, Average Loss: -153.4877\n",
            "Epoch 90, reward Loss: 0.1841\n",
            "Epoch 91, Average Loss: -153.5253\n",
            "Epoch 91, reward Loss: 0.1841\n",
            "Epoch 92, Average Loss: -153.5612\n",
            "Epoch 92, reward Loss: 0.1841\n",
            "Epoch 93, Average Loss: -153.6473\n",
            "Epoch 93, reward Loss: 0.1841\n",
            "Epoch 94, Average Loss: -153.6633\n",
            "Epoch 94, reward Loss: 0.1841\n",
            "Epoch 95, Average Loss: -153.6400\n",
            "Epoch 95, reward Loss: 0.1841\n",
            "Epoch 96, Average Loss: -153.7128\n",
            "Epoch 96, reward Loss: 0.1841\n",
            "Epoch 97, Average Loss: -153.7859\n",
            "Epoch 97, reward Loss: 0.1841\n",
            "Epoch 98, Average Loss: -153.7971\n",
            "Epoch 98, reward Loss: 0.1841\n",
            "Epoch 99, Average Loss: -153.8165\n",
            "Epoch 99, reward Loss: 0.1841\n",
            "Epoch 100, Average Loss: -153.8968\n",
            "Epoch 100, reward Loss: 0.1841\n",
            "MDN-RNN model saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6YzPnikgJyu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wHleHA4_IiIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g9oMliDLQWgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lg_8uX7dSqRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nd1vCsQDq1cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HXFy-zmDIpFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "kqgbZ3UfP4Mi",
        "outputId": "fe7b445f-00db-4521-edf9-0e865e85cc40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_85e08735-0aa4-48d8-ba9f-a2103295837a\", \"mdn_rnn_new.pth\", 1539120)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ju7hsa67USBE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}